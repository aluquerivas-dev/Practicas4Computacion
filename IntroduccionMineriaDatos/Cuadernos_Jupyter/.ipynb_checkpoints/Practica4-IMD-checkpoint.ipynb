{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/p4/portada.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo <a id='codigo'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metodos disponibles: \n",
      " \t\t -> (base) Aplicar metodo base \n",
      " \t\t -> (bagging) Aplicar Bagging \n",
      " \t\t -> (boosting_r) Aplicar Boosting Regresion \n",
      " \t\t -> (boosting_c) Aplicar Boosting Clasificacionbase\n",
      "Aplicando método base:\n",
      "\n",
      "\tDataset: 0diabetes.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.67532468 0.79220779 0.71428571 0.67532468 0.66233766 0.74025974\n",
      " 0.7012987  0.79220779 0.71052632 0.75      ]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.64935065 0.64935065 0.64935065 0.64935065 0.64935065 0.64935065\n",
      " 0.64935065 0.64935065 0.65789474 0.65789474]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.68831169 0.72727273 0.7012987  0.61038961 0.67532468 0.67532468\n",
      " 0.83116883 0.79220779 0.59210526 0.76315789]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 1glass.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.56521739 0.65217391 0.65217391 0.86363636 0.68181818 0.31818182\n",
      " 0.71428571 0.6        0.75       0.61111111]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.65217391 0.52173913 0.69565217 0.86363636 0.59090909 0.5\n",
      " 0.66666667 0.7        0.65       0.83333333]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.82608696 0.60869565 0.60869565 0.68181818 0.59090909 0.5\n",
      " 0.85714286 0.6        0.8        0.66666667]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 2ionosphere.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.80555556 0.86111111 0.80555556 0.72222222 0.80555556 0.8\n",
      " 0.91176471 0.88235294 0.88235294 0.82352941]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.94444444 0.94444444 0.94444444 0.86111111 0.91666667 0.91428571\n",
      " 0.97058824 1.         1.         0.88235294]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.80555556 0.83333333 0.94444444 0.77777778 0.77777778 0.91428571\n",
      " 0.97058824 0.91176471 1.         0.85294118]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 3ris.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[1.         0.93333333 1.         1.         0.86666667 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[1.         0.93333333 1.         1.         1.         0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[1.         0.93333333 1.         0.93333333 0.93333333 0.86666667\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 4cpu.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.0862069  0.22033898 0.37142857 0.46666667 0.66666667 0.6875\n",
      " 0.75       0.8        1.         1.        ]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.13793103 0.38983051 0.48571429 0.5        0.66666667 0.6875\n",
      " 0.75       0.8        1.         1.        ]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.17241379 0.44067797 0.51428571 0.5        0.625      0.6875\n",
      " 0.75       0.8        1.         1.        ]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 5contactLenses.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.5        0.75       0.5        0.75       0.66666667 0.\n",
      " 1.         1.         1.         1.        ]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.5        0.5        0.5        0.5        0.66666667 1.\n",
      " 1.         1.         1.         1.        ]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[1.         0.75       1.         1.         0.33333333 0.\n",
      " 1.         1.         1.         1.        ]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 6segment-challenge.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.94078947 0.91447368 0.95394737 0.95394737 0.91390728 0.94666667\n",
      " 0.88590604 0.91891892 0.91836735 0.96598639]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.59868421 0.47368421 0.51973684 0.60526316 0.55629139 0.58\n",
      " 0.51006711 0.5        0.57142857 0.54421769]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.96052632 0.94736842 0.98026316 0.96710526 0.97350993 0.98\n",
      " 0.94630872 0.97297297 0.97278912 0.89795918]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 7segment-test.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.9047619  0.9047619  0.92771084 0.8902439  0.86419753 0.95\n",
      " 0.88607595 0.86075949 0.96202532 0.92405063]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.46428571 0.35714286 0.43373494 0.37804878 0.39506173 0.375\n",
      " 0.34177215 0.32911392 0.4556962  0.4556962 ]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.95238095 0.92857143 0.95180723 0.95121951 0.9382716  0.9375\n",
      " 0.92405063 0.92405063 0.96202532 0.96202532]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 8weather.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[0.5 1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[0.66666667 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[0.5 1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "\tDataset: 9iris.arff\n",
      "\tPuntuacion KNN:\n",
      "\n",
      "\t\t\t[1.         0.93333333 1.         1.         0.86666667 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "\t----------------\n",
      "\tPuntuacion SVM:\n",
      "\n",
      "\t\t\t[1.         0.93333333 1.         1.         1.         0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "\t-----------------\n",
      "\tPuntuacion TREE:\n",
      "\n",
      "\t\t\t[1.         0.93333333 1.         0.93333333 0.93333333 0.86666667\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "\t-----------------------------------------------\n",
      "\n",
      "Test de Iman Davenport\n",
      "Valor de F: 1.0000000000000018 , valor de RESULT: nan\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import wilcoxon,rankdata,f,friedmanchisquare\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lista_datasets = listdir('./data/p4/datasets/')\n",
    "matrix_ccr   = [['---',0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],['KNN',0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],['SVM',0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],['DTC',0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]]\n",
    "def imanDavenport():\n",
    "    print('Test de Iman Davenport')\n",
    "    nDatasets=10\n",
    "    kAlgoritms=3\n",
    "    chi=friedmanchisquare(matrix_ccr.iloc[1,1:],matrix_ccr.iloc[2,1:],matrix_ccr.iloc[3,1:])\n",
    "    F=((nDatasets-1)*chi[0])/(nDatasets*(kAlgoritms-1)-chi[0])\n",
    "    RESULT = f.ppf(q=F, dfn=kAlgoritms-1, dfd=(kAlgoritms-1)*(nDatasets-1))\n",
    "    print('Valor de F: '+str(F)+' , valor de RESULT: '+str(RESULT))\n",
    "    if F < RESULT:\n",
    "        print('No hay diferencias significativas')\n",
    "    elif F > RESULT:\n",
    "        print('Si hay diferencias significativas')\n",
    "        \n",
    "def iniciarM():\n",
    "    c = 1\n",
    "    for i in lista_datasets:\n",
    "        matrix_ccr[0][c] = i\n",
    "        c = c + 1\n",
    "        \n",
    "def base():\n",
    "    AUX = 1\n",
    "    print('Aplicando método base:\\n')\n",
    "    for indice in lista_datasets:\n",
    "        print('\\tDataset: ' + str(indice))\n",
    "        dataset = arff.loadarff('./data/p4/datasets/' + str(indice))\n",
    "        df = pd.DataFrame(dataset[0])\n",
    "\n",
    "        input = df.iloc[:, df.columns != 'class']\n",
    "        output = pd.factorize(df['class'])[0]\n",
    "\n",
    "        # Llamada y entrenamiento del algoritmo KNN\n",
    "        print('\\tPuntuacion KNN:\\n')\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        cv_scores = cross_val_score(knn, input, output, cv=10)\n",
    "        print('\\t\\t\\t'+str(cv_scores))\n",
    "        matrix_ccr[1][AUX] = np.mean(cv_scores)*100\n",
    "        print('\\t----------------')\n",
    "        # llamada y entrenamiento algoritmo SVM\n",
    "        print('\\tPuntuacion SVM:\\n')\n",
    "        svm = SVC(gamma='auto')\n",
    "        cv_scores = cross_val_score(svm, input, output, cv=10)\n",
    "        print('\\t\\t\\t'+str(cv_scores))\n",
    "        matrix_ccr[2][AUX] = np.mean(cv_scores)*100\n",
    "        print('\\t-----------------')\n",
    "        # llamada y entrenamiento del arbol de decision\n",
    "        print('\\tPuntuacion TREE:\\n')\n",
    "        arbol = DecisionTreeClassifier()\n",
    "        cv_scores = cross_val_score(arbol, input, output, cv=10)\n",
    "        print('\\t\\t\\t'+str(cv_scores))\n",
    "        matrix_ccr[3][AUX] = np.mean(cv_scores)*100\n",
    "        print('\\t-----------------------------------------------\\n')\n",
    "        AUX = AUX + 1\n",
    "\n",
    "def bagging():\n",
    "    print('Aplicando metodo de combinacion BAGGING\\n')\n",
    "    for indice in lista_datasets:\n",
    "        print('Dataset: ' + str(indice))\n",
    "        dataset = arff.loadarff('./data/p4/datasets/' + str(indice))\n",
    "        df = pd.DataFrame(dataset[0])\n",
    "\n",
    "        input = df.iloc[:, df.columns != 'class']\n",
    "        output = pd.factorize(df['class'])[0]\n",
    "\n",
    "        for ESTIMADOR_BASE in [KNeighborsClassifier(n_neighbors=5), SVC(gamma='auto'), DecisionTreeClassifier()]:\n",
    "            kfold = model_selection.KFold(n_splits=10)\n",
    "            model = BaggingClassifier(base_estimator=ESTIMADOR_BASE)\n",
    "            results = model_selection.cross_val_score(model, input, output, cv=kfold)\n",
    "            print('\\n')\n",
    "            print(results)\n",
    "            print('\\n')\n",
    "\n",
    "def boosting_r():\n",
    "    print('Algoritmo de BOOSTING: Regressor\\n')\n",
    "    for indice in lista_datasets:\n",
    "        print('\\tDataset: ' + str(indice))\n",
    "        dataset = arff.loadarff('./data/p4/datasets/' + str(indice))\n",
    "        df = pd.DataFrame(dataset[0])\n",
    "\n",
    "        input = df.iloc[:, df.columns != 'class']\n",
    "        output = pd.factorize(df['class'])[0]\n",
    "\n",
    "        kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "        RE = model_selection.cross_val_score(model, input, output, cv=kfold)\n",
    "\n",
    "        print('\\n')\n",
    "        print(RE)\n",
    "        print('\\n')\n",
    "\n",
    "def boosting_c():\n",
    "    print('Algoritmo de BOOSTING: Classifier')\n",
    "    for indice in lista_datasets:\n",
    "        print('Dataset: ' + str(indice))\n",
    "        dataset = arff.loadarff('./data/p4/datasets/' + str(indice))\n",
    "        df = pd.DataFrame(dataset[0])\n",
    "\n",
    "        input = df.iloc[:, df.columns != 'class']\n",
    "        output = pd.factorize(df['class'])[0]\n",
    "\n",
    "        kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "        model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "        RE = model_selection.cross_val_score(model, input, output, cv=kfold)\n",
    "\n",
    "        print('\\n')\n",
    "        print(RE)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    iniciarM()\n",
    "    value = input('Metodos disponibles: \\n \\t\\t -> (base) Aplicar metodo base \\n \\t\\t -> (bagging) Aplicar Bagging \\n \\t\\t -> (boosting_r) Aplicar Boosting Regresion \\n \\t\\t -> (boosting_c) Aplicar Boosting Clasificacion')\n",
    "    # Obten la función del diccionario\n",
    "\n",
    "    if value == 'base':\n",
    "        base()\n",
    "        matrix_ccr = pd.DataFrame(matrix_ccr)\n",
    "        imanDavenport()\n",
    "    if value == 'bagging':\n",
    "        bagging()\n",
    "    if value == 'boosting_r':\n",
    "        boosting_r()\n",
    "    if value == 'boosting_c':\n",
    "        boosting_c()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta[1]: Seleccione tres algoritmos clasificación de los disponibles en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repuesta [1]: \n",
    "    Los clasificadores escogidos han sido:\n",
    "        - KNN\n",
    "        - SVM\n",
    "        - DTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta [2]: Para cada uno de estos tres métodos de clasificación realice los siguientes pasos usando validación cruzada de 10 particiones:\n",
    "#### 2.1. Aplique el método base a cada uno de los conjuntos y anote los resultados obtenidos.\n",
    "#### 2.2. Aplique el método de combinación de clasificadores Bagging a cada uno de los conjuntos y anote los resultados obtenidos.\n",
    "#### 2.3. Seleccione dos algoritmos de Boosting y aplique estos algoritmos a cada uno de los conjuntos y anote los resultados obtenidos.\n",
    "#### 2.4. Compare si hay diferencias significativas entre ellos usando el test de Iman-Davenport. Si es así, aplique el procedimiento de Wilcoxon para comparar cada método de agrupación con el clasificador base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta [2]:\n",
    "En el codigo se han diferenciado 4 funciones para el calculo necesario, Metodos disponibles:\n",
    "\n",
    " \t\t -> (base) Aplicar metodo base \n",
    " \t\t -> (bagging) Aplicar Bagging \n",
    " \t\t -> (boosting_r) Aplicar Boosting Regresion \n",
    " \t\t -> (boosting_c) Aplicar Boosting Clasificacion\n",
    "         \n",
    "El metodo base se encarga de hacer la cross-validation de manera simple, la forma más sencilla de utilizar la validación cruzada es llamar a la función auxiliar cross_val_score en el estimador y el conjunto de datos.\n",
    "\n",
    "El metodo bagging es un metaestimador de conjunto que se ajusta a los clasificadores de base en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales (ya sea votando o promediando) para formar una predicción final. Tal metaestimulador puede usarse típicamente como una forma de reducir la varianza de un estimador de caja negra (por ejemplo, un árbol de decisión), al introducir la aleatorización en su procedimiento de construcción y luego hacer un conjunto de él.\n",
    "\n",
    "El método de boosting o aumento de gradiente para regresión,construye un modelo aditivo de manera progresiva por etapas; Permite la optimización de funciones arbitrarias de pérdida diferenciable. En cada etapa se ajusta un árbol de regresión en el gradiente negativo de la función de pérdida dada.\n",
    "\n",
    "El método de boosting para clasificación construye un modelo aditivo de manera progresiva por etapas; Permite la optimización de funciones arbitrarias de pérdida diferenciable.En cada etapa se ajusta un árbol de regresión en el gradiente negativo de la función de pérdida dada. \n",
    "\n",
    "La gran diferencia entre las técnicas de bagging  y validación es que los modelos promedios de bagging es que son usados para reducir la varianza a la que está sujeta la predicción, mientras que el muestreo de validación como la validación cruzada y la validación fuera de la rutina evalúan un número de modelos sustitutos suponiendo que son equivalentes (es decir, un buen sustituto) para el modelo real en cuestión que está entrenado en todo el conjunto de datos.\n",
    "\n",
    "### Respuesta [2.4]:\n",
    "Despues de realizar el test de Iman-Davenport nunca han salido cambios sicnificativos, con lo cual no haremos el test de wilcoxon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta [3]:Enuncie las conclusiones del estudio indicando la influencia del clasificador base en el rendimiento de las agrupaciones de clasificadores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta [3]:\n",
    "\n",
    "Para esta práctica por primera vez en la asignatura hemos usado la metodología de validación cruzada, que consiste en dividr el dataset en mas partes para usarlos a la hora de validar.Para que el método vaya fino hay que tener en cuenta que hay que estudiar en cierta medida el número de divisiones para cada uno de los problemas.\n",
    "\n",
    "Una vez aplicado el método anterior se han aplicado métodos de ensemble, en este caso han sido Bagging y GradientBoosting. Estos métodos utilizan múltiples algoritmos de aprendizaje, y el objetivo es mejorar el rendimiento que dichos algoritmos obtienen de manera individual. Lo que ya de entrada nos da a suponer que el tiempo de computo será mucho mayor, así que no en todos los casos sería recomendable realizar un ensemble.\n",
    "\n",
    "En la etodología de Bagging este tomara un clasificador base especificado por el usuario programador, y aparte de este clasificador tambien es necesario un conjunto de parametros. Es muy importante que al usar este método se estimen bien los parametros a usar y sobre todo el clasificador base, pues de esto dependera un buen clasificador en el futuro.\n",
    "\n",
    "Por otro lado, el própio clasificador Bagging tiene sus própios parámetros que también deben estudiarse para optimizarlos según el problema. Con lo cual, podría ser una buena opción para mejorar el rendimiento (debido sobre todo a que este método tiende a reducir el sobreentrenamiento), aunque requiriendo bastante estudio por parte del programador.\n",
    "\n",
    "En los otros métodos hemos utilizado GradientBoostingRegressor y GradientBoostingClasifier, es  muy importante saber cual de los dos utilizar según el problema ya que la puntuación obtenida cambiará según nos enfrentemos a clasifiación o regresión.\n",
    "\n",
    "Comparato con los métodos de Bagging, aquí el clasificador base se construye de forma lineal, con lo que el sesgo del ensemble se va reduciend y por ello esta metodología funciona mejor combinando modelos menos complejos como resultado se obtiene un modelo muy fuerte.\n",
    "\n",
    "Los algoritmos de Boosting se dedican a construir un gran árbol en cada una de las iteraciones tomando por referencía los errores encontrados en cada una de las iteraciones realizadas, para estos metodos hay que tener clara la identificación de los valores perdidos pues suponen un gran problema para estas metodologias pues se fijan mucho en estos valores tornando así unos valores inadecuados, para que esto no ocurra se debe de realizar un buen preprocesamiento e identificación de estos valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
